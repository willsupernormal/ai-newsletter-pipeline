# AI Newsletter Pipeline

> Automated daily AI news digest with intelligent filtering, Slack integration, and Airtable content pipeline.

**Status:** âœ… Operational | **Last Updated:** October 28, 2025

## âœ¨ Features

### Core Functionality
- **31 RSS Sources**: Enterprise AI, Open Source, Tech News
- **Multi-Stage AI Filtering**: GPT-4 selects top 5 from 180+ articles
- **Daily Slack Digest**: Posted at 7 AM AEST with summaries
- **Interactive Buttons**: "Add to Pipeline" button in Slack
- **Airtable Integration**: One-click article addition to content pipeline
- **Smart Deduplication**: Prevents duplicate content
- **GitHub Actions**: Fully automated daily runs

### Recent Improvements
- âœ… Async button processing (no more timeouts)
- âœ… Visual button feedback (Processing â†’ Added)
- âœ… 17 new premium sources added
- âœ… Browser headers for better scraping
- âœ… Brotli compression support

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DAILY DIGEST FLOW                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. GitHub Actions (7 AM AEST)
   â†“
2. Scrape 31 RSS Feeds â†’ ~180 articles
   â†“
3. Stage 1 AI Filter â†’ Top 10 articles
   â†“
4. Stage 2 AI Filter â†’ Top 5 articles
   â†“
5. Post to Slack with buttons
   â†“
6. Store in Supabase

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  BUTTON CLICK FLOW                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. User clicks "Add to Pipeline" in Slack
   â†“
2. Slack â†’ Railway Webhook Server
   â†“
3. Button changes to "Processing..." (instant)
   â†“
4. Background: Fetch from Supabase â†’ Scrape full article â†’ Push to Airtable
   â†“
5. Button changes to "âœ… Added" (3-4 seconds)
   â†“
6. Article appears in Airtable
â””â”€â”€ Daily Digest Creation
    â”œâ”€â”€ AI-Generated Summary â†’ Key themes and insights
    â”œâ”€â”€ Selected Articles â†’ Top 5 with enhanced descriptions
    â””â”€â”€ Database Storage â†’ daily_digests table with article references

Boss Interaction (Anytime via Claude MCP)
â”œâ”€â”€ Daily Digest Review â†’ "Show me today's digest"
â”œâ”€â”€ Weekly Overview â†’ "What are this week's top themes?"
â”œâ”€â”€ Targeted Queries â†’ "Find vendor lock-in articles from this week"
â”œâ”€â”€ Article Selection â†’ "Mark articles 1,3,5 as newsletter-ready"
â””â”€â”€ Performance Analysis â†’ "Which sources performed best this week?"
```

## Quick Start

### 1. Environment Setup

```bash
# Clone and setup
git clone <your-repo-url>
cd ai-newsletter-pipeline
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your API keys and configuration
```

### 2. Database Setup

1. Create a new Supabase project at https://supabase.com
2. Copy your project URL and keys to `.env`
3. Run the SQL schema in Supabase SQL Editor (see `database/schema.sql`)

### 3. Service Account Setup

**OpenAI**: Get API key from https://platform.openai.com/api-keys

**Twitter Scraping** (choose one):
- **Apify**: Sign up at https://apify.com, get API token
- **RapidAPI**: Sign up at https://rapidapi.com, find Twitter API service

**Gmail**: 
- Enable 2-factor authentication
- Generate app password for the pipeline
- Tag newsletters with "newsletter" label

### 4. GitHub Actions Setup

```bash
# Add secrets to your GitHub repository
gh secret set SUPABASE_URL
gh secret set SUPABASE_KEY
gh secret set SUPABASE_SERVICE_KEY
gh secret set OPENAI_API_KEY
gh secret set TWITTER_SERVICE  # "apify" or "rapidapi"
gh secret set APIFY_API_TOKEN   # if using Apify
gh secret set RAPIDAPI_KEY      # if using RapidAPI
gh secret set GMAIL_EMAIL
gh secret set GMAIL_APP_PASSWORD
```

### 5. Boss MCP Configuration

Add to Claude Desktop MCP settings:

```json
{
  "mcpServers": {
    "supabase": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-supabase"],
      "env": {
        "SUPABASE_URL": "https://your-project.supabase.co",
        "SUPABASE_SERVICE_ROLE_KEY": "your_service_role_key"
      }
    }
  }
}
```

## Usage

### Local Testing

```bash
# Enhanced AI digest pipeline (recommended)
python run_ai_digest_pipeline.py

# Show recent daily digests
python run_ai_digest_pipeline.py show

# Legacy weekly pipeline
python main.py --dry-run  # Test run without storing
python main.py           # Full pipeline execution
python main.py --cleanup # Clean old content

# RSS-only pipeline for testing
python run_rss_pipeline.py

# Simple pipeline without AI processing
python run_simple_pipeline.py

# Test individual components
python -m scrapers.rss_scraper
python -m scrapers.twitter_scraper
python -m scrapers.gmail_scraper
```

### Boss Interaction Workflow

Once Claude MCP is configured, use natural language queries:

```
Daily Digest Review:
"Show me today's daily digest"
"What were the key insights from yesterday's digest?"
"Show me the last 3 days of digests"

Weekly Overview:
"Show me this week's summary stats and top articles"
"What are the trending themes this week?"
"Which sources performed best this week?"

Theme-Based Searching:
"Find all vendor lock-in articles from this week"
"Show me data strategy articles with high relevance scores"
"What AI governance stories do we have?"

Content Selection:
"Mark articles with IDs abc123, def456 for newsletter"
"Set article abc123 as priority 1 lead story"
"Show me all articles selected for newsletter this week"

Analytics & Performance:
"Which content sources are delivering the best articles?"
"Show me relevance score trends over the past month"
"What topics are we covering most frequently?"
```

## Content Sources

### RSS Feeds
- VentureBeat AI
- AI Business
- MIT Technology Review  
- TechCrunch AI
- The Register AI/ML
- Analytics India Magazine
- Harvard Business Review

### Twitter Accounts
- @AndrewYNg
- @karpathy
- @ylecun
- @sama
- @OpenAI
- @GoogleAI

### Gmail Newsletters
- Any email tagged with "newsletter" label
- Processed in last 24 hours
- Supports major newsletter platforms

## Content Evaluation

Articles are scored 0-100 based on alignment with "Don't panic. Prepare your data. Stay agnostic." philosophy:

- Business relevance for tech executives (30 points)
- Data strategy/vendor independence themes (25 points)
- Actionable insights vs pure research (20 points)
- Enterprise decision-making impact (15 points)
- Recency and relevance (10 points)

## Daily Process

### Automated Pipeline (Daily at 7 AM UTC)
1. **Content Scraping**: Collect from RSS feeds, Twitter, Gmail newsletters
2. **Multi-Stage AI Processing**:
   - Stage 1: Filter to top 15 most relevant articles
   - Stage 2: Select final 5 articles with enhanced summaries
3. **Daily Digest Creation**: AI generates comprehensive summary with key insights
4. **Database Storage**: Store in `daily_digests` table with article references
5. **Weekly Organization**: Articles organized by week for easy boss access

### Boss Interaction (Anytime via MCP)
1. **Daily Review**: Check morning digest for key developments
2. **Weekly Planning**: Review week's themes and top articles
3. **Newsletter Curation**: Select and prioritize articles for publication
4. **Performance Monitoring**: Track source quality and content trends
5. **Ad-hoc Queries**: Search for specific topics or themes as needed

## File Structure

```
ai-newsletter-pipeline/
â”œâ”€â”€ main.py                 # Orchestration and CLI
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ .env.example           # Environment template
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py        # Configuration management
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ rss_scraper.py     # RSS feed processing
â”‚   â”œâ”€â”€ twitter_scraper.py # Twitter API integration
â”‚   â””â”€â”€ gmail_scraper.py   # Newsletter email processing
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ supabase_client.py    # Database operations
â”‚   â”œâ”€â”€ supabase_simple.py    # Simplified client for basic ops
â”‚   â”œâ”€â”€ digest_storage.py     # Daily digest storage and retrieval
â”‚   â”œâ”€â”€ weekly_manager.py     # Week cycling logic
â”‚   â””â”€â”€ schema.sql           # Complete database schema with views
â”œâ”€â”€ processors/
â”‚   â”œâ”€â”€ content_processor.py  # Cleaning and formatting
â”‚   â”œâ”€â”€ deduplicator.py      # Duplicate detection
â”‚   â”œâ”€â”€ ai_evaluator.py      # OpenAI scoring (legacy)
â”‚   â”œâ”€â”€ multi_stage_digest.py # Two-stage AI filtering system
â”‚   â”œâ”€â”€ data_aggregator.py   # Multi-source content aggregation
â”‚   â””â”€â”€ theme_extractor.py   # Topic and theme analysis
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py          # Logging setup
â”‚   â””â”€â”€ helpers.py         # Utility functions
â””â”€â”€ .github/workflows/
    â””â”€â”€ daily-scrape.yml   # CI/CD automation
```

## Contributing

1. Fork the repository
2. Create a feature branch
3. Make changes and add tests
4. Submit a pull request

## License

MIT License - see LICENSE file for details